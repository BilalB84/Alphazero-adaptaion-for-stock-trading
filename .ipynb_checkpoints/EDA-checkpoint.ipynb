{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium.wrappers.monitoring'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_stock_model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDQN\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_dqn\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mA2C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_a2c\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPPO\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_ppo\n",
      "File \u001b[0;32m~/Thesis/DQN.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DQN.py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQN\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtradingenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StockTradingEnv\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_stock_data\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/stable_baselines3/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m A2C\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_system_info\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddpg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDPG\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/stable_baselines3/a2c/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m A2C\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CnnPolicy, MlpPolicy, MultiInputPolicy\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA2C\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spaces\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuffers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RolloutBuffer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mon_policy_algorithm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OnPolicyAlgorithm\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:17\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtype_aliases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     DictReplayBufferSamples,\n\u001b[1;32m     12\u001b[0m     DictRolloutBufferSamples,\n\u001b[1;32m     13\u001b[0m     ReplayBufferSamples,\n\u001b[1;32m     14\u001b[0m     RolloutBufferSamples,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_device\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VecNormalize\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Check memory used by replay buffer when possible\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsutil\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/__init__.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_normalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VecNormalize\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_transpose\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VecTransposeImage\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_video_recorder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VecVideoRecorder\n\u001b[1;32m     16\u001b[0m VecEnvWrapperT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVecEnvWrapperT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mVecEnvWrapper)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munwrap_vec_wrapper\u001b[39m(env: VecEnv, vec_wrapper_class: Type[VecEnvWrapperT]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[VecEnvWrapperT]:\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmonitoring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m video_recorder\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_vec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VecEnv, VecEnvObs, VecEnvStepReturn, VecEnvWrapper\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdummy_vec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium.wrappers.monitoring'"
     ]
    }
   ],
   "source": [
    "from alphazeroagent import AlphaZeroNetwork \n",
    "from tradingenv import StockTradingEnv\n",
    "from MCTS import MCTS, MCTSNode\n",
    "from Train import train_stock_model\n",
    "from evaluate import evaluate\n",
    "from DQN import train_dqn\n",
    "from A2C import train_a2c\n",
    "from PPO import train_ppo\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: gymnasium 0.29.1\n",
      "Uninstalling gymnasium-0.29.1:\n",
      "  Successfully uninstalled gymnasium-0.29.1\n",
      "Requirement already satisfied: gym in /home/vardaan/anaconda3/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/vardaan/anaconda3/lib/python3.10/site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/vardaan/anaconda3/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/vardaan/anaconda3/lib/python3.10/site-packages (from gym) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall gymnasium -y\n",
    "!pip install gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vardaan/Thesis/Train.py:146: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.29218778 -0.29317302 -0.29293292 -0.29236993 -0.29228713 -0.29080514\n",
      " -0.28467848 -0.29145093 -0.29400094 -0.29243616 -0.29121911 -0.29078031\n",
      " -0.2920222  -0.29021732 -0.2942576  -0.29222918 -0.29384364 -0.29351247\n",
      " -0.27615083 -0.29048225 -0.29156684  4.04720347  4.10719509  0.21687168\n",
      " -0.16552316  0.47603781 -0.16700515 -0.25102323 -0.25651239 -0.24511183\n",
      " -0.26820272 -0.28220298 -0.27530634 -0.26614946 -0.28218642 -0.27548021]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_data.loc[:, ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']] = scaler.fit_transform(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 1/1000, Loss: 397.8913, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vardaan/Thesis/Train.py:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  states_tensor = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 2/1000, Loss: 403.3350, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 3/1000, Loss: 394.3336, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 4/1000, Loss: 377.3283, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 5/1000, Loss: 383.5516, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 6/1000, Loss: 398.6740, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 7/1000, Loss: 397.5563, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 8/1000, Loss: 375.9106, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 9/1000, Loss: 375.9524, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 10/1000, Loss: 370.3236, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 11/1000, Loss: 393.6018, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 12/1000, Loss: 386.5325, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 13/1000, Loss: 367.1217, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 14/1000, Loss: 284.9398, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 15/1000, Loss: 312.7547, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 16/1000, Loss: 364.1339, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 17/1000, Loss: 354.8820, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 18/1000, Loss: 370.4759, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 19/1000, Loss: 303.5936, Profit: -28.6000, Reward: -25.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 20/1000, Loss: 316.4726, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 21/1000, Loss: 370.4624, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 22/1000, Loss: 386.5711, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 23/1000, Loss: 375.5011, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 24/1000, Loss: 334.9929, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 25/1000, Loss: 379.0201, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 26/1000, Loss: 372.8216, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 27/1000, Loss: 372.7531, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 28/1000, Loss: 346.1488, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 29/1000, Loss: 376.3805, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 30/1000, Loss: 209.3990, Profit: -19.6000, Reward: -16.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 31/1000, Loss: 363.6367, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 32/1000, Loss: 381.0146, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 33/1000, Loss: 284.4668, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 34/1000, Loss: 287.8143, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 35/1000, Loss: 185.8049, Profit: -17.6000, Reward: -14.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 36/1000, Loss: 373.6352, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 37/1000, Loss: 309.0783, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 38/1000, Loss: 356.0504, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 39/1000, Loss: 354.8860, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 40/1000, Loss: 354.1691, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 41/1000, Loss: 339.6012, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 42/1000, Loss: 376.2615, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 43/1000, Loss: 342.2843, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 44/1000, Loss: 367.5571, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 45/1000, Loss: 341.3984, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 46/1000, Loss: 369.3639, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 47/1000, Loss: 374.0029, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 48/1000, Loss: 356.3326, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 49/1000, Loss: 373.0414, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 50/1000, Loss: 333.3594, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 51/1000, Loss: 309.8736, Profit: -30.6000, Reward: -27.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 52/1000, Loss: 228.9882, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 53/1000, Loss: 360.5996, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 54/1000, Loss: 298.0480, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 55/1000, Loss: 365.0891, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 56/1000, Loss: 365.0220, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 57/1000, Loss: 337.2434, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 58/1000, Loss: 104.6375, Profit: -9.6000, Reward: -6.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 59/1000, Loss: 330.2900, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 60/1000, Loss: 361.7892, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 61/1000, Loss: 332.9876, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 62/1000, Loss: 332.6812, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 63/1000, Loss: 360.8968, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 64/1000, Loss: 318.0203, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 65/1000, Loss: 350.9273, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 66/1000, Loss: 349.7041, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 67/1000, Loss: 361.8675, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 68/1000, Loss: 334.7091, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 69/1000, Loss: 362.2666, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 70/1000, Loss: 227.8150, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 71/1000, Loss: 330.4049, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 72/1000, Loss: 366.4146, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 73/1000, Loss: 292.8948, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 74/1000, Loss: 365.8989, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 75/1000, Loss: 341.2915, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 76/1000, Loss: 236.3914, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 77/1000, Loss: 253.5125, Profit: -23.6000, Reward: -20.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 78/1000, Loss: 296.2402, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 79/1000, Loss: 340.5655, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 80/1000, Loss: 359.2359, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 81/1000, Loss: 364.4518, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 82/1000, Loss: 354.2423, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 83/1000, Loss: 340.0127, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 84/1000, Loss: 276.5180, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 85/1000, Loss: 356.1655, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 86/1000, Loss: 345.9166, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 87/1000, Loss: 346.9756, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 88/1000, Loss: 358.1097, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 89/1000, Loss: 338.9895, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 90/1000, Loss: 358.5960, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 91/1000, Loss: 267.2801, Profit: -28.6000, Reward: -25.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 92/1000, Loss: 345.1195, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 93/1000, Loss: 357.5294, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 94/1000, Loss: 287.0996, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 95/1000, Loss: 354.7775, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 96/1000, Loss: 320.3743, Profit: -32.6000, Reward: -29.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 97/1000, Loss: 286.7849, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 98/1000, Loss: 344.4548, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 99/1000, Loss: 352.1877, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 100/1000, Loss: 314.4527, Profit: -32.6000, Reward: -29.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 101/1000, Loss: 221.9632, Profit: -23.6000, Reward: -20.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 102/1000, Loss: 351.7021, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 103/1000, Loss: 357.2628, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 104/1000, Loss: 266.0528, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 105/1000, Loss: 328.4894, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 106/1000, Loss: 340.2925, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 107/1000, Loss: 335.4265, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 108/1000, Loss: 356.8459, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 109/1000, Loss: 253.8368, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 110/1000, Loss: 270.8956, Profit: -28.6000, Reward: -25.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 111/1000, Loss: 340.0174, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 112/1000, Loss: 270.2986, Profit: -28.6000, Reward: -25.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 113/1000, Loss: 353.3756, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 114/1000, Loss: 218.8491, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 115/1000, Loss: 306.3642, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 116/1000, Loss: 360.5136, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 117/1000, Loss: 353.0956, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 118/1000, Loss: 57.3530, Profit: 0.4000, Reward: 4.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 119/1000, Loss: 295.1697, Profit: -30.6000, Reward: -27.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 120/1000, Loss: 343.0384, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 121/1000, Loss: 350.6491, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 122/1000, Loss: 356.0272, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 123/1000, Loss: 359.9693, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 124/1000, Loss: 355.9072, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 125/1000, Loss: 344.1312, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 126/1000, Loss: 289.0278, Profit: -30.6000, Reward: -27.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 127/1000, Loss: 307.4815, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 128/1000, Loss: 342.6711, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 129/1000, Loss: 355.4966, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 130/1000, Loss: 339.2224, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 131/1000, Loss: 342.5440, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 132/1000, Loss: 127.1692, Profit: -17.6000, Reward: -14.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 133/1000, Loss: 355.3434, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 134/1000, Loss: 325.3920, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 135/1000, Loss: 295.3069, Profit: -30.6000, Reward: -27.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 136/1000, Loss: 342.3479, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 137/1000, Loss: 350.1742, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 138/1000, Loss: 359.1942, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 139/1000, Loss: 355.3050, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 140/1000, Loss: 352.0003, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 141/1000, Loss: 350.0598, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 142/1000, Loss: 355.2007, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 143/1000, Loss: 335.7172, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 144/1000, Loss: 295.0468, Profit: -30.6000, Reward: -27.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 145/1000, Loss: 255.1863, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 146/1000, Loss: 349.8965, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 147/1000, Loss: 229.0394, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 148/1000, Loss: 328.0784, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 149/1000, Loss: 267.8295, Profit: -28.6000, Reward: -25.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 150/1000, Loss: 286.9616, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 151/1000, Loss: 294.8441, Profit: -30.6000, Reward: -27.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 152/1000, Loss: 343.3806, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 153/1000, Loss: 134.9395, Profit: -15.6000, Reward: -12.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 154/1000, Loss: 242.1441, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 155/1000, Loss: 321.1176, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 156/1000, Loss: 335.5119, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 157/1000, Loss: 219.5043, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 158/1000, Loss: 343.2922, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 159/1000, Loss: 263.9413, Profit: -28.6000, Reward: -25.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 160/1000, Loss: 338.6569, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 161/1000, Loss: 354.8965, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 162/1000, Loss: 343.2398, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 163/1000, Loss: 285.7386, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 164/1000, Loss: 178.5711, Profit: -21.6000, Reward: -18.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 165/1000, Loss: 312.1709, Profit: -32.6000, Reward: -29.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 166/1000, Loss: 354.7724, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 167/1000, Loss: 358.5118, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 168/1000, Loss: 354.8100, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 169/1000, Loss: 316.3201, Profit: -32.6000, Reward: -29.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 170/1000, Loss: 358.4506, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 171/1000, Loss: 226.1681, Profit: -24.6000, Reward: -21.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 172/1000, Loss: 351.4594, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 173/1000, Loss: 341.7452, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 174/1000, Loss: 358.3722, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 175/1000, Loss: 214.9242, Profit: -24.6000, Reward: -21.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 176/1000, Loss: 71.3230, Profit: -5.6000, Reward: -2.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 177/1000, Loss: 354.7090, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 178/1000, Loss: 327.7830, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 179/1000, Loss: 203.0494, Profit: -23.6000, Reward: -20.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 180/1000, Loss: 335.2870, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 181/1000, Loss: 335.2800, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 182/1000, Loss: 354.6373, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 183/1000, Loss: 335.2633, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 184/1000, Loss: 315.0958, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 185/1000, Loss: 226.0683, Profit: -24.6000, Reward: -21.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 186/1000, Loss: 310.6682, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 187/1000, Loss: 351.3135, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 188/1000, Loss: 266.2787, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 189/1000, Loss: 210.3755, Profit: -21.6000, Reward: -18.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 190/1000, Loss: 358.2020, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 191/1000, Loss: 354.5885, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 192/1000, Loss: 248.7725, Profit: -27.6000, Reward: -24.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 193/1000, Loss: 326.2201, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 194/1000, Loss: 343.0283, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 195/1000, Loss: 284.3456, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 196/1000, Loss: 349.5135, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 197/1000, Loss: 206.8585, Profit: -19.6000, Reward: -16.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 198/1000, Loss: 338.4204, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 199/1000, Loss: 354.5543, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 200/1000, Loss: 358.1329, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 201/1000, Loss: 338.4018, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 202/1000, Loss: 311.9840, Profit: -32.6000, Reward: -29.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 203/1000, Loss: 358.1134, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 204/1000, Loss: 314.9531, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 205/1000, Loss: 351.2151, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 206/1000, Loss: 349.4800, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 207/1000, Loss: 351.2050, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 208/1000, Loss: 358.0810, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 209/1000, Loss: 349.4707, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 210/1000, Loss: 327.6701, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 211/1000, Loss: 358.0623, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 212/1000, Loss: 335.1693, Profit: -34.6000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 213/1000, Loss: 284.2910, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 214/1000, Loss: 354.5195, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 215/1000, Loss: 351.1714, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 216/1000, Loss: 326.1601, Profit: -33.6000, Reward: -30.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 217/1000, Loss: 358.0347, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 218/1000, Loss: 287.9999, Profit: -30.6000, Reward: -27.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 219/1000, Loss: 241.7463, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 220/1000, Loss: 349.4449, Profit: -36.6000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 221/1000, Loss: 358.0204, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 222/1000, Loss: 354.4899, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 223/1000, Loss: 338.3421, Profit: -35.6000, Reward: -32.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 224/1000, Loss: 132.3571, Profit: -17.6000, Reward: -14.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 225/1000, Loss: 292.0843, Profit: -29.6000, Reward: -26.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 226/1000, Loss: 247.4350, Profit: -23.6000, Reward: -20.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 227/1000, Loss: 252.1159, Profit: -25.6000, Reward: -22.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 228/1000, Loss: 354.4793, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 229/1000, Loss: 354.4869, Profit: -37.6000, Reward: -34.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 230/1000, Loss: 357.9980, Profit: -38.6000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 35, resetting to 0.\n",
      "Iteration 231/1000, Loss: 306.3479, Profit: -31.6000, Reward: -28.0000\n",
      "Environment reset. Starting at step index: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stock_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/vardaan/Thesis/Final stocks/AACG.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m agent, train_env, mcts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stock_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstock_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstock_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2019\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training data months\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Number of features\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Hidden layer size in the model\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Number of possible actions (e.g., Buy, Sell, Hold)\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAACG.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m evaluate(agent, stock_file, year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2019\u001b[39m, mcts\u001b[38;5;241m=\u001b[39mmcts, test_months\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m], num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, load_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAACG.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Thesis/Train.py:154\u001b[0m, in \u001b[0;36mtrain_stock_model\u001b[0;34m(stock_file, year, train_months, input_size, hidden_size, action_size, num_iterations, batch_size, save_path)\u001b[0m\n\u001b[1;32m    151\u001b[0m agent \u001b[38;5;241m=\u001b[39m AlphaZeroNetwork(input_size, hidden_size, action_size)\n\u001b[1;32m    152\u001b[0m mcts \u001b[38;5;241m=\u001b[39m MCTS(agent, num_simulations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m total_losses, total_profits, total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m plot_training_progress(total_losses, total_profits, total_rewards, num_iterations)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent, train_env, mcts\n",
      "File \u001b[0;32m~/Thesis/Train.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, env, mcts, num_iterations, batch_size, save_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     41\u001b[0m     root \u001b[38;5;241m=\u001b[39m MCTSNode(state)\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([child\u001b[38;5;241m.\u001b[39mvisit_count \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[1;32m     45\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m action_probs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(action_probs)\n",
      "File \u001b[0;32m~/Thesis/MCTS.py:76\u001b[0m, in \u001b[0;36mMCTS.run\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     73\u001b[0m policy, value \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten(), value\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_expanded:\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Backpropagation with value clipping\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(search_path):\n",
      "File \u001b[0;32m~/Thesis/MCTS.py:23\u001b[0m, in \u001b[0;36mMCTSNode.expand\u001b[0;34m(self, action_probs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpand\u001b[39m(\u001b[38;5;28mself\u001b[39m, action_probs):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(action_probs):\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren[action] \u001b[38;5;241m=\u001b[39m MCTSNode(state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, parent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, action\u001b[38;5;241m=\u001b[39maction)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "stock_file = \"/home/vardaan/Thesis/Final stocks/AACG.csv\"\n",
    "agent, train_env, mcts = train_stock_model(\n",
    "        stock_file=stock_file,\n",
    "        year=2019,\n",
    "        train_months=[2, 3, 4],  # Training data months\n",
    "        input_size=7,               # Number of features\n",
    "        hidden_size=256,            # Hidden layer size in the model\n",
    "        action_size=3,              # Number of possible actions (e.g., Buy, Sell, Hold)\n",
    "        num_iterations=1000,\n",
    "        batch_size=32,\n",
    "        save_path=\"AACG.pth\"\n",
    ")\n",
    "\n",
    "evaluate(agent, stock_file, year=2019, mcts=mcts, test_months=[3], num_episodes=500, load_path=\"AACG.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vardaan/Thesis/Train.py:146: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.66396526  0.93391276 -0.22180301 -0.37875644 -0.10293647 -0.70966953\n",
      " -1.2412917   0.57890749 -0.60639205 -0.17822452  0.35605488  2.21310091\n",
      "  0.63382347  3.2809511   1.91141729  0.28218402  0.48519601 -0.82251302\n",
      "  0.07155464 -0.16440694  2.63134814 -0.55466474  0.05631989  0.17607216\n",
      " -0.41666618 -0.89957255 -0.33021779 -0.33588654 -0.79753511  0.49671066\n",
      " -0.17893311 -0.96742037  0.08785229 -1.29691628 -0.50116594 -1.40958262\n",
      " -0.51462921 -0.48291966 -0.18265322 -0.23668347]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_data.loc[:, ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']] = scaler.fit_transform(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 1/1000, Loss: 524.9445, Profit: -40.0000, Reward: -36.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 2/1000, Loss: 513.7732, Profit: -39.0000, Reward: -35.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 3/1000, Loss: 529.9940, Profit: -41.0000, Reward: -37.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 4/1000, Loss: 533.9236, Profit: -42.0000, Reward: -38.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 5/1000, Loss: 489.9819, Profit: -37.0000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 6/1000, Loss: 473.1438, Profit: -35.0000, Reward: -31.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 7/1000, Loss: 277.0845, Profit: -25.0000, Reward: -21.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 8/1000, Loss: 509.2124, Profit: -40.0000, Reward: -36.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 9/1000, Loss: 523.8739, Profit: -41.0000, Reward: -37.0000\n",
      "Environment reset. Starting at step index: 0\n",
      "End of data reached at step: 39, resetting to 0.\n",
      "Iteration 10/1000, Loss: 484.9592, Profit: -37.0000, Reward: -33.0000\n",
      "Environment reset. Starting at step index: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stock_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/vardaan/Thesis/Final stocks/A.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m agent, train_env, mcts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stock_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstock_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstock_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2019\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training data months\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Number of features\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Hidden layer size in the model\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Number of possible actions (e.g., Buy, Sell, Hold)\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m evaluate(agent, stock_file, year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2019\u001b[39m, mcts\u001b[38;5;241m=\u001b[39mmcts, test_months\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m6\u001b[39m], num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, load_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Thesis/Train.py:154\u001b[0m, in \u001b[0;36mtrain_stock_model\u001b[0;34m(stock_file, year, train_months, input_size, hidden_size, action_size, num_iterations, batch_size, save_path)\u001b[0m\n\u001b[1;32m    151\u001b[0m agent \u001b[38;5;241m=\u001b[39m AlphaZeroNetwork(input_size, hidden_size, action_size)\n\u001b[1;32m    152\u001b[0m mcts \u001b[38;5;241m=\u001b[39m MCTS(agent, num_simulations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m total_losses, total_profits, total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m plot_training_progress(total_losses, total_profits, total_rewards, num_iterations)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent, train_env, mcts\n",
      "File \u001b[0;32m~/Thesis/Train.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, env, mcts, num_iterations, batch_size, save_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     41\u001b[0m     root \u001b[38;5;241m=\u001b[39m MCTSNode(state)\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([child\u001b[38;5;241m.\u001b[39mvisit_count \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[1;32m     45\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m action_probs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(action_probs)\n",
      "File \u001b[0;32m~/Thesis/MCTS.py:61\u001b[0m, in \u001b[0;36mMCTS.run\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_expanded:\n\u001b[1;32m     60\u001b[0m     dynamic_c_puct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_puct \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(search_path) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_c_puct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     search_path\u001b[38;5;241m.\u001b[39mappend(node)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Expansion\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/MCTS.py:29\u001b[0m, in \u001b[0;36mMCTSNode.select_child\u001b[0;34m(self, c_puct)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_child\u001b[39m(\u001b[38;5;28mself\u001b[39m, c_puct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     best_child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "stock_file = \"/home/vardaan/Thesis/Final stocks/A.csv\"\n",
    "agent, train_env, mcts = train_stock_model(\n",
    "        stock_file=stock_file,\n",
    "        year=2019,\n",
    "        train_months=[2, 3],  # Training data months\n",
    "        input_size=7,               # Number of features\n",
    "        hidden_size=256,            # Hidden layer size in the model\n",
    "        action_size=3,              # Number of possible actions (e.g., Buy, Sell, Hold)\n",
    "        num_iterations=1000,\n",
    "        batch_size=32,\n",
    "        save_path=\"A.pth\"\n",
    ")\n",
    "\n",
    "evaluate(agent, stock_file, year=2019, mcts=mcts, test_months=[6], num_episodes=500, load_path=\"A.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step before reset: 0\n",
      "Environment reset. Starting at step index: 7762\n",
      "DataFrame shape: (40, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vardaan/Thesis/Train.py:131: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.01792153 -0.27349087  0.94017175  0.0259237  -0.46887721  0.04592913\n",
      "  0.70944248  0.73344899  1.66370138  4.01367229  0.93083588  1.94044313\n",
      "  0.41602954  0.83614352 -0.02809095 -0.43953592 -0.05609855 -0.74761951\n",
      " -1.01369169 -0.07810452  0.2893285  -0.33750824 -0.76629124  0.62541968\n",
      "  0.22197689 -0.50422013 -1.02969604 -0.76695809 -1.02769549 -0.27215717\n",
      " -0.87232    -0.9156651  -0.65292715 -0.4995522   1.33427867 -0.60958205\n",
      " -0.73294886 -1.03436397 -0.75562168 -0.86165044]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_data.loc[:, ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']] = scaler.fit_transform(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid step index: 7762. It should be within the range [0, 39].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stock_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/vardaan/Thesis/Final stocks/AB.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m agent, train_env, mcts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stock_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstock_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstock_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2019\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training data months\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Number of features\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Hidden layer size in the model\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Number of possible actions (e.g., Buy, Sell, Hold)\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAB.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m evaluate(agent, stock_file, year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2019\u001b[39m, mcts\u001b[38;5;241m=\u001b[39mmcts, test_months\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m], num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, load_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAB.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Thesis/Train.py:145\u001b[0m, in \u001b[0;36mtrain_stock_model\u001b[0;34m(stock_file, year, train_months, input_size, hidden_size, action_size, num_iterations, batch_size, save_path)\u001b[0m\n\u001b[1;32m    142\u001b[0m mcts \u001b[38;5;241m=\u001b[39m MCTS(agent, num_simulations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m total_losses, total_profits, total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Plotting the training progress\u001b[39;00m\n\u001b[1;32m    148\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/Thesis/Train.py:39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, env, mcts, num_iterations, batch_size, save_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Self-play to generate training data\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     states, mcts_probs, rewards \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m---> 39\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     cumulative_profit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \n",
      "File \u001b[0;32m~/Thesis/tradingenv.py:102\u001b[0m, in \u001b[0;36mStockTradingEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment reset. Starting at step index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/tradingenv.py:39\u001b[0m, in \u001b[0;36mStockTradingEnv._get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_observation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Check if self.current_step is within the valid range of the DataFrame index\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf):\n\u001b[0;32m---> 39\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid step index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. It should be within the range [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m].\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_worth\n\u001b[1;32m     50\u001b[0m         ])\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid step index: 7762. It should be within the range [0, 39]."
     ]
    }
   ],
   "source": [
    "\n",
    "stock_file = \"/home/vardaan/Thesis/Final stocks/AB.csv\"\n",
    "agent, train_env, mcts = train_stock_model(\n",
    "        stock_file=stock_file,\n",
    "        year=2019,\n",
    "        train_months=[2, 3],  # Training data months\n",
    "        input_size=7,               # Number of features\n",
    "        hidden_size=256,            # Hidden layer size in the model\n",
    "        action_size=3,              # Number of possible actions (e.g., Buy, Sell, Hold)\n",
    "        num_iterations=1000,\n",
    "        batch_size=32,\n",
    "        save_path=\"AB.pth\"\n",
    ")\n",
    "\n",
    "evaluate(agent, stock_file, year=2019, mcts=mcts, test_months=[3], num_episodes=500, load_path=\"AB.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step before reset: 0\n",
      "Environment reset. Starting at step index: 0\n",
      "DataFrame shape: (61, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vardaan/Thesis/Train.py:131: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.46882248 -0.21367829 -0.06565099 -0.50777184  3.05572442  3.91634737\n",
      "  1.62796158 -0.11225949  1.82233836  1.49015263 -0.53776704 -0.72532954\n",
      " -0.66391462 -0.40478668 -0.53700236 -0.96875069 -0.21137808 -0.01882666\n",
      "  1.52493941  1.84002467 -0.05733201 -1.09655096 -0.06124792 -0.40923293\n",
      " -0.86934226 -0.17656663 -0.36331511 -0.55262897 -0.4416825   1.39788532\n",
      "  0.1759818  -0.38049575 -0.413229    1.16907943  1.4565067  -0.38505916\n",
      "  1.45915841  0.66844204  0.43602241 -0.57574205 -0.98182426 -0.27374269\n",
      " -0.59910796 -0.638489   -0.61128735 -0.91448306  0.05096274 -0.3400972\n",
      " -0.43861145 -0.03629714 -0.38792672 -1.04146315 -0.83319262 -0.36391329\n",
      " -0.40069811 -0.64408226 -0.55319632 -0.31489975 -0.89891812 -0.28346769\n",
      " -0.31746513]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  train_data.loc[:, ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']] = scaler.fit_transform(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'KeyError: 0 when accessing step 0 in the DataFrame.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Thesis/tradingenv.py:43\u001b[0m, in \u001b[0;36mStockTradingEnv._get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOpen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbalance,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholdings,\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_worth\n\u001b[1;32m     50\u001b[0m     ])\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/pandas/core/indexing.py:1183\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/pandas/core/frame.py:4221\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   4218\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   4219\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   4220\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[0;32m-> 4221\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stock_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/vardaan/Thesis/Final stocks/ABEV.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m agent, train_env, mcts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stock_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstock_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstock_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2019\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training data months\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Number of features\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Hidden layer size in the model\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Number of possible actions (e.g., Buy, Sell, Hold)\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mABEV.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m evaluate(agent, stock_file, year\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2019\u001b[39m, mcts\u001b[38;5;241m=\u001b[39mmcts, test_months\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m], num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, load_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mABEV.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Thesis/Train.py:145\u001b[0m, in \u001b[0;36mtrain_stock_model\u001b[0;34m(stock_file, year, train_months, input_size, hidden_size, action_size, num_iterations, batch_size, save_path)\u001b[0m\n\u001b[1;32m    142\u001b[0m mcts \u001b[38;5;241m=\u001b[39m MCTS(agent, num_simulations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m total_losses, total_profits, total_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Plotting the training progress\u001b[39;00m\n\u001b[1;32m    148\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/Thesis/Train.py:39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, env, mcts, num_iterations, batch_size, save_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Self-play to generate training data\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     states, mcts_probs, rewards \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m---> 39\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     cumulative_profit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \n",
      "File \u001b[0;32m~/Thesis/tradingenv.py:102\u001b[0m, in \u001b[0;36mStockTradingEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment reset. Starting at step index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Thesis/tradingenv.py:53\u001b[0m, in \u001b[0;36mStockTradingEnv._get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when accessing step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'KeyError: 0 when accessing step 0 in the DataFrame.'"
     ]
    }
   ],
   "source": [
    "\n",
    "stock_file = \"/home/vardaan/Thesis/Final stocks/ABEV.csv\"\n",
    "agent, train_env, mcts = train_stock_model(\n",
    "        stock_file=stock_file,\n",
    "        year=2019,\n",
    "        train_months=[2, 3, 4],  # Training data months\n",
    "        input_size=7,               # Number of features\n",
    "        hidden_size=256,            # Hidden layer size in the model\n",
    "        action_size=3,              # Number of possible actions (e.g., Buy, Sell, Hold)\n",
    "        num_iterations=1000,\n",
    "        batch_size=32,\n",
    "        save_path=\"ABEV.pth\"\n",
    ")\n",
    "\n",
    "evaluate(agent, stock_file, year=2019, mcts=mcts, test_months=[3], num_episodes=500, load_path=\"ABEV.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stock_file = \"/home/vardaan/Thesis/Final stocks/ACBI.csv\"\n",
    "agent, train_env, mcts = train_stock_model(\n",
    "        stock_file=stock_file,\n",
    "        year=2019,\n",
    "        train_months=[2, 3, 4],  # Training data months\n",
    "        input_size=7,               # Number of features\n",
    "        hidden_size=256,            # Hidden layer size in the model\n",
    "        action_size=3,              # Number of possible actions (e.g., Buy, Sell, Hold)\n",
    "        num_iterations=1000,\n",
    "        batch_size=32,\n",
    "        save_path=\"ACBI.pth\"\n",
    ")\n",
    "\n",
    "evaluate(agent, stock_file, year=2019, mcts=mcts, test_months=[3], num_episodes=500, load_path=\"ACBI.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step before reset: 0\n",
      "Environment reset. Starting at step index: 0\n",
      "DataFrame shape: (8056, 7)\n",
      "Current step before increment: 0\n",
      "Current step after increment: 1\n",
      "Step: 1, Reward: 0.0, Done: False\n",
      "Current step before increment: 1\n",
      "Current step after increment: 2\n",
      "Step: 2, Reward: 246.875, Done: False\n",
      "Current step before increment: 2\n",
      "Current step after increment: 3\n",
      "Step: 3, Reward: 246.875, Done: False\n",
      "Current step before increment: 3\n",
      "Current step after increment: 4\n",
      "Step: 4, Reward: 370.3125, Done: False\n",
      "Current step before increment: 4\n",
      "Current step after increment: 5\n",
      "Step: 5, Reward: 370.3125, Done: False\n",
      "Current step before increment: 5\n",
      "Current step after increment: 6\n",
      "Step: 6, Reward: 370.3125, Done: False\n",
      "Current step before increment: 6\n",
      "Current step after increment: 7\n",
      "Step: 7, Reward: 370.3125, Done: False\n",
      "Current step before increment: 7\n",
      "Current step after increment: 8\n",
      "Step: 8, Reward: 4.3125, Done: False\n",
      "Current step before increment: 8\n",
      "Current step after increment: 9\n",
      "Step: 9, Reward: 4.3125, Done: False\n",
      "Current step before increment: 9\n",
      "Current step after increment: 10\n",
      "Step: 10, Reward: 4.3125, Done: False\n"
     ]
    }
   ],
   "source": [
    "stock_file = \"/home/vardaan/Thesis/Final stocks/AB.csv\"\n",
    "df = pd.read_csv(stock_file)    \n",
    "env = StockTradingEnv(df)\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(10):  # Run for 10 steps\n",
    "    action = env.action_space.sample()  # Random action for testing\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(f\"Step: {env.current_step}, Reward: {reward}, Done: {done}\")\n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode finished, resetting environment.\")\n",
    "        obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m stock_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/vardaan/Thesis/Final stocks/AB.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_dqn\u001b[49m(stock_file, \u001b[38;5;241m2019\u001b[39m, [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mABDQN.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dqn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "stock_file = \"/home/vardaan/Thesis/Final stocks/AB.csv\"\n",
    "\n",
    "train_dqn(stock_file, 2019, [2, 3], 7, 256, 3, 1000, 32, \"ABDQN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
